"""
> Importing Libraries
"""

import spacy
import re
import sys
import os
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from keras.layers import LSTM,Dense,Flatten
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow import keras
from progressbar import progressbar
import pandas as pd
import numpy as np
pb = progressbar(2)

"""> Extracting Zip files to Locations

    > postive data to POS/pos Folder
    
    > Negetive Data to NEG/neg Folder
"""

import zipfile

pos_zip="pos.zip"

neg_zip="neg.zip"


with zipfile.ZipFile(pos_zip, 'r') as zip_ref:
    
    zip_ref.extractall("/POS")
  
with zipfile.ZipFile(neg_zip, 'r') as zip_ref:
    
    zip_ref.extractall("/NEG")

"""# read_text function takes input folder locations of data


```
def read_text(folder_locations,max_files_toread): 
  returns list[list[str],list[int]
```


"""

def read_text(folder_locations,max_files_toread):
    text_data=[]

    if type(folder_locations)!=str:

        text_file_Locations=[]

        for index,location in enumerate(folder_locations):

            text_file_Locations.append( [ os.path.join(location,file_name) for file_name in os.listdir(location)][:max_files_toread]  ) 

    else:

        print(" Folder locations in list or tuple format  \n\t ex:[Folder loc1,Folder loc2,Folder loc3]")

        return

    class_names = [0 for _ in folder_locations]

    present_length = 0

    for location_ind,text_file_Location in enumerate(text_file_Locations):

        for index,text_file in enumerate(text_file_Location):

            pb.print(index,len(text_file_Location))

            with open(text_file,'r',encoding='utf-8') as f:

                try:

                    text_data.append(f.readlines())

                except Exception as e:

                    print(e,text_file)

        class_names[location_ind]=len(text_data)-present_length

        present_length = len(text_data)

    class_labels=[]

    for index,no_of_files_inclass in enumerate(class_names):

        class_labels+=[index for number in range(no_of_files_inclass)]

    return text_data,class_labels

text_data,class_labels = read_text(["/NEG/neg","/POS/pos"],5000)

text_data[1],class_labels[1]

""">Removing HTML tags from text data"""

regex = re.compile(r'<[^>]+>')

def remove_html(string):

    return regex.sub(' ', string)
    
text_data=[remove_html(text[0]) for text in text_data]

text_data[0]

"""# Data tokenizer"""

def tokenize(texts):

    for text_ind,text in enumerate(texts):

        texts[text_ind]=text.lower()
        
    nlp = spacy.load('en_core_web_sm') 
    
    tokenized_texts = []

    for ind,text in enumerate(texts):

        pb.print(ind,len(texts))

        doc = nlp(text) 

        tokens = [token.text for token in doc]  

        tokenized_texts.append(tokens)
        
    return tokenized_texts

tokenized_texts = tokenize( text_data )

"""# Calculating frequent_tokens"""

def freqent_tokens( text_data, frequency=5 ):

    counter={}

    for text in text_data:

        for token in text:

            if token in counter:

                counter[token]+=1

            else:

                counter[token]=1

    freq_tokens=[]

    for i in counter.items():
      
      if i[1]>frequency:

        freq_tokens.append(i[0])


    return freq_tokens

freq_tokens = freqent_tokens(tokenized_texts,frequency=300)

f" Most frequent Tokens length  {len(freq_tokens)}"

tokenized_texts[1][:10]

"""# Padding sequences"""

def Pad_sequences(sequences):
    avg_len=0

    for text in sequences:

        avg_len+=len(text)

    avg_len=int(avg_len/len(tokenized_texts))

    for text_ind,text in enumerate(sequences):

        if len(text)>=avg_len:

            sequences[text_ind]=text[:avg_len]

        else:

            for i in range(avg_len-len(text)):
              
              sequences[text_ind].append('PAD')
              
Pad_sequences(tokenized_texts)

"""# converting words to one hot coding vectors"""

def one_hot_coding(tokenized_texts):
    
    token_to_number={x:to_categorical(ind,num_classes=len(freq_tokens)+1,dtype='uint8') for ind,x in enumerate(freq_tokens)}

    token_to_number['unk']=np.array([0 for x in range(len(freq_tokens))]+[1])

    for text in tokenized_texts:

      for token_ind,token in enumerate(text):

        if token in token_to_number:

          text[token_ind]=token_to_number[token]

        else:

          text[token_ind]=token_to_number['unk']
      
one_hot_coding(tokenized_texts)

X = np.array(tokenized_texts,dtype='uint8')

Data_set_1=X

"""# Reading tweets Dataset"""

data2=pd.read_csv("2013semeval_train.csv")

data2.tweet

"""#Cleaning unicodes out of text"""

unicode_escape_regex = re.compile(r'\\u([0-9a-fA-F]{4})')

def convert_escape_sequence(match):
    return chr(int(match.group(1), 16))


result = unicode_escape_regex.sub(convert_escape_sequence, data2.tweet[1])
def change_string(string):
    return unicode_escape_regex.sub(convert_escape_sequence,string)
print(result)

data2.tweet=data2.tweet.apply(change_string)

"""#Tokenizing"""

tokenized_texts = tokenize( data2.tweet )

"""#Calculating freq tokens"""

freq_tokens = freqent_tokens(tokenized_texts,frequency=200)

f" Most frequent Tokens length  {len(freq_tokens)}"

"""#Padding and converting to one_hot_coding"""

Pad_sequences(tokenized_texts)      
one_hot_coding(tokenized_texts)

X = np.array(tokenized_texts,dtype='uint8')

Data_set_2=X

Data_set_1.shape,Data_set_2.shape

"""# Convertin dataset_1 labels to numpy array"""

Data_set_1_labels=np.array(class_labels)

"""#Converting dataset_2 labels to numpy array"""

unique_labels={x:ind for ind,x in enumerate(data2.label.unique())}

data2.label = data2.label.apply(lambda x:unique_labels[x])

Data_set_2_labels = data2.label.values

"""#Final Data """

Data_set_1.shape,Data_set_1_labels.shape,Data_set_2.shape,Data_set_2_labels.shape

"""# Recurrent Neural Network Models"""

model_1_RNN=keras.Sequential([
    LSTM(256,input_shape=Data_set_1.shape[1:]),
    Dense(2,activation='sigmoid')
])
model_1_RNN.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model_2_RNN=keras.Sequential([
    LSTM(256,input_shape=Data_set_2.shape[1:]),
    Dense(3,activation='sigmoid')
])
model_2_RNN.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

from sklearn.model_selection import train_test_split

x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(Data_set_1, Data_set_1_labels, test_size=0.25, random_state=42)
x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(Data_set_2, Data_set_2_labels, test_size=0.25, random_state=42)

history_1 = model_1_RNN.fit(x_train_1,y_train_1,epochs=10)

history_2 = model_2_RNN.fit(x_train_2,y_train_2,epochs=10)

from sklearn.metrics import precision_recall_fscore_support
y_pred_1=model_1_RNN.predict(x_test_1)
scores=precision_recall_fscore_support(np.argmax(y_pred_1,axis=1),y_test_1,average='macro')
print(f"""\n\nRNN Model_1 For DataSet1
    Precision = {scores[0]}
    Recall    = {scores[1]}
    f1_score  = {scores[2]}
""")

from sklearn.metrics import precision_recall_fscore_support
y_pred_2=model_2_RNN.predict(x_test_2)
scores=precision_recall_fscore_support(np.argmax(y_pred_2,axis=1),y_test_2,average='macro')
print(f"""\n\nRNN Model_2 For DataSet_2
    Precision = {scores[0]}
    Recall    = {scores[1]}
    f1_score  = {scores[2]}
""")

history_1.history

plt.plot([x for x in range(10)],history_1.history['loss'],c='r')
plt.title("Epochs vs loss for dataset _1")
plt.x_label="Epochs"
plt.y_label="Loss"
plt.show()
plt.plot([x for x in range(10)],history_1.history['accuracy'],c='y')
plt.title("Epochs vs accuracy for dataset _1")
plt.x_label="Epochs"
plt.y_label="Loss"
plt.show()
plt.plot([x for x in range(10)],history_2.history['loss'],c='r')
plt.title("Epochs vs loss for dataset _2")
plt.x_label="Epochs"
plt.y_label="Loss"
plt.show()
plt.plot([x for x in range(10)],history_2.history['accuracy'],c='y')
plt.title("Epochs vs accuracy for dataset _2")
plt.x_label="Epochs"
plt.y_label="Loss"
plt.show()
